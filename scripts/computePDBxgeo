#!/usr/bin/env python3
__author__ = "Antonio Marinho da Silva Neto"
__license__ = "MIT License"
__version__ = "1.0_dev"
__maintainer__ = "Antonio Marinho da Silva Neto"
__email__ = "amarinhosn@pm.me"
__status__ = "Alpha"

import pandas as pd
import multiprocessing as mp
import os
import ssaxgeo.PDBx as PDBx
import argparse


def decompressGZ(gzfile):
    os.system(f"gunzip {gzfile}")


def decompressGZFiles(mylocalpdb_pat, ncpus=1):

    # get list of files to unzip
    def getGZFilesList(base_dir):
        # get files
        res = []
        for (dir_path, dir_names, file_names) in os.walk(base_dir):
            res.extend([f for f in file_names if f.endswith(".gz")])
        # add paths it assumes pdb_chain subdir structure (ex: 1F09 would be at subdir F0)
        gz_flpaths = [base_dir+f"{fl[1:3]}/"+fl for fl in res]
        return gz_flpaths

    pool = mp.Pool(ncpus)
    gz_list = getGZFilesList(mylocalpdb_path+'pdb_chain/')
    print(f"    -> {len(gz_list)} files to decompress")
    pool.map(decompressGZ, gz_list)

def compute_xgeo(kwargs):
    #TODO check if xgeo file exists
    # if exists, then just return the file
    pdb_chain_flpth = kwargs["pdb_chain_flpth"]
    output_dir = kwargs["output_dir"]
    try:
        # assemble pdb chain filepath
        prefix = pdb_chain_flpth.split("/")[-1].split(".")[0]
        chain_output_dir = f"{output_dir}/{prefix[1:3]}"
        # if xgeo csv exists, skip it
        chain_xgeo_flpath = chain_output_dir+"/"+prefix+".csv"
        if os.path.exists(chain_xgeo_flpath):
            pass
        # if not, compute xgeo and write data
        else:
            os.makedirs(chain_output_dir, exist_ok=True)
            entry = PDBx.entry(pdb_chain_flpth)
            entry.xdata_df.to_csv(chain_xgeo_flpath)
    # TODO investigate those erorrs
    except:
        print(f"Error: {pdb_chain_flpth}")
    
def addPDBChainsFlPaths(sampled_clstrd_pdb_df, mylocalpdb):
    '''
    add a new column with pdb chains path and excludes collumns
    with missing files
    '''

    def getPathFromIdx(i, mylocalpdb):
        '''get pdb chain paths based on indexes'''
        ch_dir = i[1:3]
        ch_pdb = i+".pdb"
        return f"{mylocalpdb}pdb_chain/{ch_dir}/{ch_pdb}"
    # get list of indexes with missing files
    missing_i = []
    chain_flpath_list = []
    index = sampled_clstrd_pdb_df.index
    for i_fl in [[i, getPathFromIdx(i, mylocalpdb_path)] for i in sampled_clstrd_pdb_df.index]:
        chain_flpath_list.append(i_fl[1])
        if os.path.isfile(i_fl[1]) == False:
            missing_i.append(i_fl[0])
    # create new column
    sampled_clstrd_pdb_df["chain_flpath"] = chain_flpath_list 
    # exclude columns with missing files
    missing_fls_i = sampled_clstrd_pdb_df.loc[missing_i].index
    if len(missing_fls_i) >0:
        print(f"{len(missing_fls_i)} files which chains were not found at {mylocalpdb}")
    return sampled_clstrd_pdb_df.drop(missing_fls_i, axis=0)

def get_all_files_in_dir(directory):
    file_paths = []
    for root, dirs, files in os.walk(directory):
        for file in files:
            file_path = os.path.join(root, file)
            file_paths.append(file_path)
    return file_paths


parser = argparse.ArgumentParser(description='Compute xgeo data for a given set of chain provided.')
parser.add_argument('--mylocalpdb_path', type=str, required=True,
                    help='path to a localpdb database')
parser.add_argument('--sampled_clstrd_path', type=str, required=True,
                    help='path to a sampled clustered csv (produced by getSampleOfCLstrPDB)')
parser.add_argument('--xgeo_output_dir', type=str, default=None, 
                    help='path of a dir to store xgeo csv files (default = xgeo_output_dir+"/xgeo_chains/"')
parser.add_argument('--ncpus', type=int, default=1,
                    help='Number of cpus to be used (default=1)')
parser.add_argument('--out_csv', type=str, default='/home/antonio/Projects/HlxCnt/sampled_clustered_pdb_2.csv', help='Description of out_csv')

args = parser.parse_args()
args_dct = vars(args)

mylocalpdb_path = args_dct["mylocalpdb_path"]
sampled_clstrd_path = args_dct["sampled_clstrd_path"]
if args_dct["xgeo_output_dir"]:
    xgeo_output_dir = args_dct["xgeo_output_dir"]
ncpus = args_dct["ncpus"]
out_csv = args_dct["out_csv"]

# --- SANITY CHECKS ---
errors = []

try:
    assert(os.path.exists(mylocalpdb_path))
except(AssertionError):
    errors.append(f"ERROR: {mylocalpdb_path} does not exist")

try:
    assert(os.path.exists(sampled_clstrd_path))
except(AssertionError):
    errors.append(f"ERROR: {sampled_clstrd_path} does not exist")

try:
    assert(os.path.exists(xgeo_output_dir))
except(AssertionError):
    errors.append(f"ERROR: {xgeo_output_dir} does not exist")

try:
    assert(type(ncpus) == int)
except(AssertionError):
    errors.append(f"ERROR: {ncpus} is not int")

try:
    assert(os.path.exists(out_csv))
except(AssertionError):
    errors.append(f"ERROR: {out_csv} does not exist")


if len(errors) > 0:
    for err in errors:
        print(err)
    exit(1)
# -----------------------


print("@ decompress chain pdb.gz files...")
# decompress chain gz files if needed
decompressGZFiles(mylocalpdb_path, ncpus=ncpus)

print(f"@ loading {sampled_clstrd_path} ... ")
# add paths per entry
# load samples clustered pdb
sampled_clstrd_pdb_df = pd.read_csv(sampled_clstrd_path, index_col=0)
sampled_clstrd_pdb_df = addPDBChainsFlPaths(sampled_clstrd_pdb_df, mylocalpdb_path)
print(f"    -> {len(sampled_clstrd_pdb_df)} total entries")
print("@ Compute pdb chain xgeo descriptors...")
# run melodia on each entry, store xgeo.csv files at mylocalpdb dir
# check if file already exist to avoid recomputing xgeo data!
chain_fls = sampled_clstrd_pdb_df["chain_flpath"].values
kwargs_to_proc = []
for i in chain_fls:
    kwargs_to_proc.append({"pdb_chain_flpth":i, "output_dir":xgeo_output_dir})

pool = mp.Pool(ncpus)
pool.map(compute_xgeo, kwargs_to_proc)


print(f"@ writing {out_csv} ...")
# add xgeo files paths at a new column
files_found = get_all_files_in_dir(xgeo_output_dir)
paths_idx_dct = {}

for f in files_found:
    paths_idx_dct[f.split("/")[-1].split(".")[0]] = f
sampled_clstrd_pdb_df["xgeo_chain_flpath"] = sampled_clstrd_pdb_df.index.map(paths_idx_dct)
missing_xgeo_N = len(sampled_clstrd_pdb_df.loc[sampled_clstrd_pdb_df["xgeo_chain_flpath"].isna()])
print(f"    -> {missing_xgeo_N} (out of {len(sampled_clstrd_pdb_df)} with not xgeo data")

# update sample clustered pdb csv
sampled_clstrd_pdb_df.to_csv(out_csv)
print(":: DONE ::")

#TODO next thing to do is to adapt PreapreBCX_df to new structure
# Prepare BCX_df get a set of structures and identify fragments 