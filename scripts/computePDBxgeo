#!/usr/bin/env python3
__author__ = "Antonio Marinho da Silva Neto"
__license__ = "MIT License"
__version__ = "1.0_dev"
__maintainer__ = "Antonio Marinho da Silva Neto"
__email__ = "amarinhosn@pm.me"
__status__ = "Alpha"

import pandas as pd
import multiprocessing as mp
import os
import ssaxgeo.PDBx as PDBx
import argparse


# --- | FUNCTIONS | ---
# Sanitize pdb functions
def __add_cryst1_record(input_content):


    # Check if the file already contains a CRYST1 record
    has_cryst1 = any(line.startswith('CRYST1') for line in input_content)

    if not has_cryst1:
        # Create a new CRYST1 record
        cryst1_record = 'CRYST1{:9.3f}{:9.3f}{:9.3f}{:7.2f}{:7.2f}{:7.2f} P 1           \n'.format(0.0, 0.0, 0.0, 90.0, 90.0, 90.0)

        # Add the CRYST1 record to the content
        input_content.insert(0, cryst1_record)
        # Write the modified content to the output file
    return input_content

def __exclude_hetatoms(input_content):
    output_content = []
    for line in input_content:
        if line.startswith("HETATM"):
            continue
        else:
            output_content.append(line)
    return output_content



def __find_residues_with_missing_ca(pdb_file_path):
    with open(pdb_file_path, 'r') as pdb_file:
        atom_lines = [line for line in pdb_file if line.startswith('ATOM')]

    res_CA_dct = {}
    # necessary to compute dihedral angle
    res_C_dct = {}
    res_N_dct = {}

    missing_ca_residues = []

    for line in atom_lines:
        atom_name = line[12:16].strip()
        residue_name = line[17:20].strip()
        res_number = line[22:26].strip()
        
        # add keys to dict if absent 
        if res_number not in res_CA_dct.keys():
            res_CA_dct[res_number] = False
        
        if res_number not in res_C_dct.keys():
            res_C_dct[res_number] = False

        if res_number not in res_N_dct.keys():
            res_N_dct[res_number] = False

        # set true if atoms are present
        if atom_name == 'CA':
            res_CA_dct[res_number] = True
        if atom_name == 'C':
            res_C_dct[res_number] = True
        if atom_name == 'N':
            res_N_dct[res_number] = True
        
    for res in res_CA_dct.keys():
        if res_CA_dct[res] == False:
            missing_ca_residues.append(res)
            continue
        if res_C_dct[res] == False:
            missing_ca_residues.append(res)
            continue
        if res_N_dct[res] == False:
            missing_ca_residues.append(res)
            continue
    return missing_ca_residues
def __delete_residues(content, residues_to_exclude):
    output_content = []
    for line in content:
        if line.startswith('ATOM'):
            residue_number = line[22:26].strip()
            if residue_number not in residues_to_exclude:
                output_content.append(line)
        else:
            output_content.append(line)
    return output_content

def __exclude_letter_post_resnumber(input_content, remove=True, fix=False):
    """
    melodia raises index error when a letter is adjacent to
    residue number. so it must be removed
    ex:
    The line bellow crashes Melodia
    "ATOM    377  N   GLY A  80A      24.355 -13.413  10.541  1.00 14.63           N  "
    The line bellow don't
    "ATOM    377  N   GLY A  80       24.355 -13.413  10.541  1.00 14.63           N  "

    """
    output_content = []
    for line in input_content:
        if line.startswith("ATOM"):
            code_for_insert = line[22:29].strip()
            
            if code_for_insert.isdigit() == False:
                if fix == True:
                    new_line = line[:26]+" "+line[27:]
                    output_content.append(new_line)
                    continue
                if remove == True:
                    continue
            else:
                output_content.append(line)
        else:
            output_content.append(line)
    return output_content

def validate_pdb_file(pdb_file):

    errors_found = []
    # get list os res with missing CA atoms
    res_miss_ca_lst = __find_residues_with_missing_ca(pdb_file)

    with open(pdb_file, 'r') as f:
        content = f.readlines()

    # Check if the file starts with the "HEADER" record
    #if not content[0].startswith('HEADER'):
    #    return False

    # Check if the file contains at least one "ATOM" or "HETATM" record
    has_atom_record = any(line.startswith('ATOM') or line.startswith('HETATM') for line in content)
    if not has_atom_record:
        errors_found.append("No ATOM or HETATM records present")

    # Check if the file ends with the "END" record
    if not content[-1].startswith('END'):
        errors_found.append("No END at the end of file")

    # Check if the "MODEL" and "ENDMDL" records, if present, occur in pairs
    model_count = sum(1 for line in content if line.startswith('MODEL'))
    endmdl_count = sum(1 for line in content if line.startswith('ENDMDL'))
    if model_count != endmdl_count:
        errors_found.append("At least one MODEL missing ENDMDL")

    # Check if the "CRYST1" record is present
    has_cryst1_record = any(line.startswith('CRYST1') for line in content)
    if not has_cryst1_record:
        errors_found.append("No CRYST1 record is present")
    return errors_found, res_miss_ca_lst

def write_sanitized_pdb(errors_found_lst, input_pdb, output_pdb, res_miss_ca_lst):
    # check possible errors at input pdb
    if len(errors_found_lst) == 0:
        return False
    
    with open(input_pdb, 'r') as f:
        content = f.readlines()
    # exclude hetatoms
    content = __exclude_hetatoms(content)
    # remove extra letter post resnumber
    content = __exclude_letter_post_resnumber(content) 
    
    if "No CRYST1 record is present" in errors_found_lst:
        # add cryst1 record if needed
        content = __add_cryst1_record(content)
    # remove residues with missing CA
    if len(res_miss_ca_lst) > 0:
        print(f"WARN: residues with missing CA, C or N {res_miss_ca_lst} from {input_pdb} will not be considered")
        content = __delete_residues(content, res_miss_ca_lst)

    with open(output_pdb, 'w') as f:
        f.writelines(content)
        
# --- decompress gz files functions
def decompressGZ(gzfile):
    os.system(f"gunzip {gzfile}")


def decompressGZFiles(mylocalpdb_pat, ncpus=1):

    # get list of files to unzip
    def getGZFilesList(base_dir):
        # get files
        res = []
        for (dir_path, dir_names, file_names) in os.walk(base_dir):
            res.extend([f for f in file_names if f.endswith(".gz")])
        # add paths it assumes pdb_chain subdir structure (ex: 1F09 would be at subdir F0)
        gz_flpaths = [base_dir+f"{fl[1:3]}/"+fl for fl in res]
        return gz_flpaths

    pool = mp.Pool(ncpus)
    gz_list = getGZFilesList(mylocalpdb_path+'pdb_chain/')
    print(f"    -> {len(gz_list)} files to decompress")
    pool.map(decompressGZ, gz_list)

# --- Compute data from pdbs functions

def compute_xgeo(kwargs):
    #TODO check if xgeo file exists
    # if exists, then just return the file
    pdb_chain_flpth = kwargs["pdb_chain_flpth"]
    output_dir = kwargs["xgeo_output_dir"]
    try:
        # assemble pdb chain filepath
        prefix = pdb_chain_flpth.split("/")[-1].split(".")[0]
        chain_output_dir = f"{output_dir}/{prefix[1:3]}"

        # if xgeo csv exists, skip it
        chain_xgeo_flpath = chain_output_dir+"/"+prefix+".csv"
        if os.path.exists(chain_xgeo_flpath):
            pass
        # if not, compute xgeo and write data
        else:
            os.makedirs(chain_output_dir, exist_ok=True)
            entry = PDBx.entry(pdb_chain_flpth)
            entry.xdata_df.to_csv(chain_xgeo_flpath)
    # TODO investigate those erorrs
    except:
        print(f"Error: {pdb_chain_flpth}")

def compute_dssp_data(kwargs):
    pdb_chain_flpth = kwargs["pdb_chain_flpth"]
    output_dir = kwargs["dssp_output_dir"]
    
    # assemble pdb chain filepath
    prefix = pdb_chain_flpth.split("/")[-1].split(".")[0]
    chain_output_dir = f"{output_dir}/{prefix[1:3]}"
    # if xgeo csv exists, skip it
    chain_dssp_flpath = chain_output_dir+"/"+prefix+".dssp"
    if os.path.exists(chain_dssp_flpath):
        pass
    else:
        os.makedirs(chain_output_dir, exist_ok=True)
        cmd = f"mkdssp --output-forma dssp {pdb_chain_flpth} > {chain_dssp_flpath}"
        os.system(cmd)

# --- | handle paths 
def addPDBChainsFlPaths(sampled_clstrd_pdb_df, mylocalpdb):
    '''
    add a new column with pdb chains path and excludes collumns
    with missing files
    '''

    def getPathFromIdx(i, mylocalpdb):
        '''get pdb chain paths based on indexes'''
        ch_dir = i[1:3]
        ch_pdb = i+".pdb"
        return f"{mylocalpdb}pdb_chain/{ch_dir}/{ch_pdb}"
    # get list of indexes with missing files
    missing_i = []
    chain_flpath_list = []
    lst_of_paths = [[i, getPathFromIdx(i, mylocalpdb_path)] for i in sampled_clstrd_pdb_df.index]
    total_paths = len(lst_of_paths)
    for i, i_fl in enumerate(lst_of_paths):
        print(f"{i} / {total_paths}", end="\r")
        # check if there is a sanitized version, use that instead
        sanitized_path = i_fl[1].split(".")[0]+"_sntzd.pdb"
        if os.path.isfile(sanitized_path):
            chain_flpath_list.append(sanitized_path)
            continue
        # if the chain file is missing, record as a missing value
        if os.path.isfile(i_fl[1]) == False:
            missing_i.append(i_fl[0])
            chain_flpath_list.append(i_fl[1])
        # if the file exist and there is no sanitized file,
        # write one if needed
        else:
            errs_found_lst, missing_ca_res_lst = validate_pdb_file(i_fl[1])
            # if the pdb file has no ATOM or HETATOM, consider as missing
            if "No ATOM or HETATM records present" in errs_found_lst:
                print(f"ERROR: No ATOM or HETATM records present at {i_fl[1]}")
                missing_i.append(i_fl[0])
                chain_flpath_list.append(i_fl[1])
                continue
            # sanitize pdb and store as the file to be used
            write_sanitized_pdb(errs_found_lst, i_fl[1], sanitized_path, missing_ca_res_lst)
            chain_flpath_list.append(sanitized_path)
            continue

    # create new column
    sampled_clstrd_pdb_df["chain_flpath"] = chain_flpath_list 
    # exclude columns with missing files
    missing_fls_i = sampled_clstrd_pdb_df.loc[missing_i].index
    if len(missing_fls_i) >0:
        print(f"{len(missing_fls_i)} files which chains were not found at {mylocalpdb}")
    return sampled_clstrd_pdb_df.drop(missing_fls_i, axis=0)

def get_all_files_in_dir(directory):
    file_paths = []
    for root, dirs, files in os.walk(directory):
        for file in files:
            file_path = os.path.join(root, file)
            file_paths.append(file_path)
    return file_paths

# add chain
def get_chain(row_chflpt):
    return row_chflpt.split("/")[-1].split(".")[0].split("_")[1]

parser = argparse.ArgumentParser(description='Compute xgeo data for a given set of chain provided.')
parser.add_argument('--mylocalpdb_path', type=str, required=True,
                    help='path to a localpdb database')
parser.add_argument('--sampled_clstrd_path', type=str, required=True,
                    help='path to a sampled clustered csv (produced by getSampleOfCLstrPDB)')
parser.add_argument('--xgeo_output_dir', type=str, default=None, 
                    help='path of a dir to store xgeo csv files (default = mylocalpdb_path+"/xgeo_chains/"')
parser.add_argument('--ncpus', type=int, default=1,
                    help='Number of cpus to be used (default=1)')
parser.add_argument('--out_csv', type=str, default='/home/antonio/Projects/HlxCnt/sampled_clustered_pdb_2.csv', help='Description of out_csv')

parser.add_argument('--dssp_output_dir', type=str, default=None, 
                    help='path of a dir to store dssp files (default = mylocalpdb_path+"/dssp/"')

args = parser.parse_args()
args_dct = vars(args)

mylocalpdb_path = os.path.abspath(args_dct["mylocalpdb_path"])+"/"
sampled_clstrd_path = os.path.abspath(args_dct["sampled_clstrd_path"])
if args_dct["xgeo_output_dir"]:
    xgeo_output_dir = os.path.abspath(args_dct["xgeo_output_dir"])+"/"
else:
    xgeo_output_dir = os.path.abspath(mylocalpdb_path+"/xgeo/")+"/"
if args_dct["dssp_output_dir"]:
    dssp_output_dir = os.path.abspath(args_dct["dssp_output_dir"])+"/"
else:
    dssp_output_dir = os.path.abspath(mylocalpdb_path+"/dssp/")+"/"

ncpus = args_dct["ncpus"]
out_csv = os.path.abspath(args_dct["out_csv"])

print(">> Parameters:")
print(f"  mylocalpdb_path     : {mylocalpdb_path}")
print(f"  sampled_clstrd_path : {sampled_clstrd_path}")
print(f"  xgeo_output_dir     : {xgeo_output_dir}")
print(f"  dssp_output_dir     : {dssp_output_dir}")
print(f"  ncpus               : {ncpus}")
print(f"  out_csv             : {out_csv}")
print()



# --- SANITY CHECKS ---
errors = []

try:
    assert(os.path.exists(mylocalpdb_path))
except(AssertionError):
    errors.append(f"ERROR: {mylocalpdb_path} does not exist")

try:
    assert(os.path.exists(sampled_clstrd_path))
except(AssertionError):
    errors.append(f"ERROR: {sampled_clstrd_path} does not exist")

try:
    assert(os.path.exists(xgeo_output_dir))
except(AssertionError):
    errors.append(f"ERROR: {xgeo_output_dir} does not exist")

try:
    assert(os.path.exists(dssp_output_dir))
except(AssertionError):
    errors.append(f"ERROR: {dssp_output_dir} does not exist")

try:
    assert(type(ncpus) == int)
except(AssertionError):
    errors.append(f"ERROR: {ncpus} is not int")

#try:
#    assert(os.path.exists(out_csv))
#except(AssertionError):
#    errors.append(f"ERROR: {out_csv} does not exist")


if len(errors) > 0:
    for err in errors:
        print(err)
    exit(1)
# -----------------------


print("@ decompress chain pdb.gz files...")
# decompress chain gz files if needed
decompressGZFiles(mylocalpdb_path, ncpus=ncpus)

print(f"@ loading {sampled_clstrd_path} ... ")
# add paths per entry
# load samples clustered pdb
sampled_clstrd_pdb_df = pd.read_csv(sampled_clstrd_path, index_col=0)
sampled_clstrd_pdb_df = addPDBChainsFlPaths(sampled_clstrd_pdb_df, mylocalpdb_path)
print(f"    -> {len(sampled_clstrd_pdb_df)} total entries")
# run melodia on each entry, store xgeo.csv files at mylocalpdb dir
# check if file already exist to avoid recomputing xgeo data!
chain_fls = sampled_clstrd_pdb_df["chain_flpath"].values
kwargs_to_proc = []
assert(len(chain_fls)>0), "ERROR: no chains files found."
for i in chain_fls:
    kwargs_to_proc.append({"pdb_chain_flpth":i, "xgeo_output_dir":xgeo_output_dir, "dssp_output_dir":dssp_output_dir})

pool = mp.Pool(ncpus)
print("@ Compute dssp data...")
pool.map(compute_dssp_data, kwargs_to_proc)
print("@ Compute pdb chain xgeo descriptors...")
pool.map(compute_xgeo, kwargs_to_proc)
pool.close()

print(f"@ writing {out_csv} ...")
# add xgeo files paths at a new column
xgeo_files_found = get_all_files_in_dir(xgeo_output_dir)
dssp_files_found = get_all_files_in_dir(dssp_output_dir)
xgeo_paths_idx_dct = {}
dssp_paths_idx_dct = {}

for f in xgeo_files_found:
    if f.endswith("sntzd.csv"):
        pdb_code = "_".join(f.split("/")[-1].split(".")[0].split("_")[:2])
    else:
        pdb_code = f.split("/")[-1].split(".")[0]
    xgeo_paths_idx_dct[pdb_code] = f

for f in dssp_files_found:
    if f.endswith("sntzd.dssp"):
        pdb_code = "_".join(f.split("/")[-1].split(".")[0].split("_")[:2])
    else:
        pdb_code = f.split("/")[-1].split(".")[0]
    dssp_paths_idx_dct[pdb_code] = f

sampled_clstrd_pdb_df["xgeo_chain_flpath"] = sampled_clstrd_pdb_df.index.map(xgeo_paths_idx_dct)
sampled_clstrd_pdb_df["dssp_chain_flpath"] = sampled_clstrd_pdb_df.index.map(dssp_paths_idx_dct)
missing_xgeo_N = len(sampled_clstrd_pdb_df.loc[sampled_clstrd_pdb_df["xgeo_chain_flpath"].isna()])
missing_dssp_N = len(sampled_clstrd_pdb_df.loc[sampled_clstrd_pdb_df["dssp_chain_flpath"].isna()])

print(f"    -> {missing_xgeo_N} with no xgeo data. (out of {len(sampled_clstrd_pdb_df)})")
print(f"    -> {missing_dssp_N} with no dssp data. (out of {len(sampled_clstrd_pdb_df)})")

# add chain col
sampled_clstrd_pdb_df["chain"] = sampled_clstrd_pdb_df["chain_flpath"].apply(get_chain)
# update sample clustered pdb csv
sampled_clstrd_pdb_df.to_csv(out_csv)
print(":: DONE ::")

#TODO next thing to do is to adapt PreapreBCX_df to new structure
# Prepare BCX_df get a set of structures and identify fragments 