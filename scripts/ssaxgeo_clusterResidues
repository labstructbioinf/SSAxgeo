#!/usr/bin/env python3
__author__ = "Antonio Marinho da Silva Neto"
__license__ = "MIT License"
__version__ = "1.0_dev"
__maintainer__ = "Antonio Marinho da Silva Neto"
__email__ = "amarinhosn@pm.me"
__status__ = "Alpha"

import ssaxgeo.PDBx as PDBx
import pandas as pd
import numpy as np
import random
import pickle as pck
import seaborn as sns
import matplotlib.pyplot as plt
##
import ssaxgeo.MathToolBox as MathToolBox
from scipy.cluster.hierarchy import fcluster
import hdbscan
##
import multiprocessing as mp
##
import argparse
import os
##


### FUNCTIONS ##################################################################

# ~~! paralelization !~~
def norm_entries(entry):
    # ---- custom values that worked before --------------
    # These values were obtained from a smaller dataset
    c_max = 2.0464588
    c_min = 0.0
    t_max = 0.1688680999999992
    t_min = -0.3012673000000001
    # ----------------------------------------------------
    #print(entry.xdata_df.columns)
    # Normalized KT
    entry.get_normalize_xdf(c_norm=True, k_min=c_min, k_max=c_max, t_min=t_min,
                            t_max=t_max)
    assert(entry.xdata_df is not None), f"ERROR: for entry {entry.pdbid}_{entry.chain}."
    return entry

def smooth_entries(entry):
    # Smooth KTW
    entry.add_smoothed('curv_norm')
    entry.add_smoothed('tor_norm')
    entry.add_smoothed('wri')
    assert(entry.xdata_df is not None), f"ERROR: for entry {entry.pdbid}_{entry.chain}."

    return entry

def standardize_coords(entry):
    entry.standardize_coords()
    return entry

def get_labels(entry):
    entry.get_dist2canonical(pi_df=pi_df, alpha_df=alpha_df, three_df=three_df, pp2_df=pp2_df)
    entry.get_labels(dist_min=0.2, pp2_max=0.07)
    assert(entry.xdata_df is not None), f"ERROR: for entry {entry.pdbid}_{entry.chain}."

    return entry

def detect_helices(entry):
    entry.detect_hlx(show_plot=False, 
                     selected_cols=['curv_norm_smooth', 'tor_norm_smooth', 'wri_smooth'],
                     label=False)
    assert(entry.xdata_df is not None), f"ERROR: for entry {entry.pdbid}_{entry.chain}."
    return entry


# ~~! PANDAS APPLY !~~
def load_entries(row):
    entry = PDBx.entry(coord_flpath=row['chain_flpath'],
            xgeo_flpath=row['xgeo_chain_flpath'],
            pdbid=row['pdb'], chain=row['chain'])
    entry.load_dssp_data(row['dssp_chain_flpath'])
    assert(entry.xdata_df is not None), f"ERROR: for entry {entry.pdbid}_{entry.chain}."

    return entry

def get_labels(entry):
    entry.get_dist2canonical(pi_df=pi_df, alpha_df=alpha_df, three_df=three_df, pp2_df=pp2_df)
    entry.get_labels(dist_min=0.2, pp2_max=0.07)
    entry.detect_hlx(show_plot=False, selected_cols=['curv_norm_smooth', 'tor_norm_smooth', 'wri_smooth'])
    assert(entry.xdata_df != None), f"ERROR: for entry {entry.pdbid}_{entry.chain}."

    return entry

#-------------------------------------------------------------------------------


desc = '''
This script load precompute data from a lPDB copy, compute normalized and smoothed values for xgeo 
descriptors for each entry, cluster residues and generate "fragments" and, optionally, run the SSEx 
assignment using pre-computed canonical regions.

'''
script_dir = os.path.dirname(os.path.realpath(__file__))


parser = argparse.ArgumentParser(description=desc)
parser.add_argument('lpdb_csv',type=str,help='path to a localPDB metadata csv')
parser.add_argument('bc_group_col', type=str,
   help="column name with cluster number for each entry (ex. 'bc-90' for BC90)")
parser.add_argument('-ncpus', type=int, default=1,
    help='number of cpus to use (default=1)')
parser.add_argument('-save_dir', type=str, default=os.getcwd(),
    help='set directory for output files (default=Working dir)')
parser.add_argument("--do_res_labeling", action="store_true",
                    help="Do residues labeling" )
parser.add_argument('-canonical_dir',type=str,default=script_dir+'/canonical/',
    help='''
set directory of canonical regions to be loaded
(default=canonical folder at this script directory)
''')

args = parser.parse_args()

NCPUS = args.ncpus
lpdb_csv_pth = args.lpdb_csv
save_dir = args.save_dir
bc_group = args.bc_group_col
CANONICAL_DIR = args.canonical_dir
do_res_labeling = args.do_res_labeling


### INPUTS #####################################################################
print('----- INPUTS ----------------------------------------------------------')
print(': NCPUS           = ', NCPUS)
print(': lPDB_CSV_PTH    = ', lpdb_csv_pth)
print(': SAVE_DIR        = ', save_dir)
print(': BC_GROUP        = ', bc_group)
print(': CANONICAL_DIR   = ', CANONICAL_DIR)
print(f': DO_RES_LABELING = {[do_res_labeling]}')
print('-----------------------------------------------------------------------')

# load lpdb data
print(f'@ loading data from {lpdb_csv_pth}...')

df = pd.read_csv(lpdb_csv_pth, index_col=0)
print(f"  > {len(df)} total entries found")

# drop rows with nan values on any col
print("   - drop rows with NAN values")
df_filtered = df.dropna()
print(f"   > {len(df_filtered.dropna())} remains")
# loading pdbx entries
grp = PDBx.group('test')
print("@ loading individual entries data...")
grp.entries = df_filtered.apply(load_entries, axis=1)
print('>> ', len(grp.entries), 'total loaded entries')

# 1 - Normalization
print('@ normalize data...')
workers = mp.Pool(processes=NCPUS)
output = workers.map(norm_entries,grp.entries.values)
workers.close()

# 2 - smoothing
print('@ smoothing data...')
workers = mp.Pool(processes=NCPUS)
output = workers.map(smooth_entries,output)
workers.close()

# 3 - standardize C alpha coordinates
#print('@ standardize coordinates...')
#workers = mp.Pool(processes=NCPUS)
#output = workers.map(standardize_coords,output)
#workers.close()

print('@ cluster residues...')
workers = mp.Pool(processes=NCPUS)
output = workers.map(detect_helices,output)
workers.close()


# 4 - get labels
if do_res_labeling == True:
    print("@ labeling residues...")
    # ~~! SS label algorithm !~~~
    print("    > loading canonical dataframes...")
    alpha_df = pck.load(open(CANONICAL_DIR+'alpha_can.p', 'rb'))
    pi_df = pck.load(open(CANONICAL_DIR+'pi_can.p', 'rb'))
    three_df = pck.load(open(CANONICAL_DIR+'three_can.p', 'rb'))
    pp2_df = pck.load(open(CANONICAL_DIR+'pp2_can.p', 'rb'))

    workers = mp.Pool(processes=NCPUS)
    output = workers.map(get_labels,output)
    workers.close()

# store at group object
grp.entries = output

# 5 - loading group dataframes
print('@ loading group dataframes...')
grp.load_grp_dfs()

# 6 - save outputs
print('@ saving group object...')
pck.dump(grp, open(save_dir+'/grp_state_'+bc_group+'.p', 'wb'))
print(' --> at ', save_dir+'/grp_state_'+bc_group+'.p')
# save raw data
print('@ saving residues dataframe...')
grp.grp_df.to_csv(save_dir+'/grp_res_df_'+bc_group+'.csv', index=False)
#pck.dump(grp.grp_df, open(save_dir+'/grp_res_df_'+bc_group+'.p', 'wb'))
print(' --> at ', save_dir+'/grp_res_df_'+bc_group+'.csv')

grp_frag_df = grp.grp_frag_df.drop(columns=[0,1])
grp_frag_df.to_csv(save_dir+'/grp_frag_df_'+bc_group+'.csv', index=False)

#pck.dump(grp.grp_frag_df, open(save_dir+'/grp_frag_df_'+bc_group+'.p', 'wb'))
print(' --> at', save_dir+'/grp_frag_df_'+bc_group+'.csv')
print(':: DONE ::')
